{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT5196-S2-2019 assessment 2\n",
    "\n",
    "\n",
    "Date: 12/09/2019\n",
    "\n",
    "Environment: Python 3.6.5 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* pandas 0.19.2 (for data frame, included in Anaconda Python 3.6) \n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* pdfminer (used to extract information from the pdf documents)\n",
    "* nltk (natural language toolkit used for processing natural language(human language))\n",
    "* io (used for dealing various I/O operations)\n",
    "* os (operating system library used to access folder and get pdfs)\n",
    "* requests (used to download the pdf files from the given link)\n",
    "* itertools (used for handling iterators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assessment we are given with a pdf document which has the download links for 200 other pdf files, on which we need to perform text pre-processing and feature extraction. In below task we have performed various [Text preprocessing and Feature Extraction](#sec_1) steps. Various processing steps like tokenization, stemming, normalizing, stopwards removal has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing and Feature Extraction <a class=\"anchor\" id=\"sec_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text analysis section we have perfomed various pre-processing steps after reading the data from the 200 pdf files. After performing the preprocessing we have extracted the features from the given document, the steps for these are been followed in the following manner.\n",
    "1. Extract the data from the pdf files.\n",
    "2. Sentence Normalization \n",
    "3. Word Tokenization\n",
    "4. Stopwords Removal\n",
    "5. Unigram creation\n",
    "6. Bigram creation\n",
    "7. Retokenization\n",
    "8. Removing Context Dependent and Rare Tokens\n",
    "9. Stemming\n",
    "10. Generating Vocabulary Index File \n",
    "11. Generating Sparse Count Vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from the pdf files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing the required libraries\n",
    "2. Converting the pdf file to text and downloading the pdfs\n",
    "3. Extracting the information from pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98K7yyqBRZau"
   },
   "source": [
    "#### Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDcvsiS_QOZD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/srikarmanthatti/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfminer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import glob,os\n",
    "import requests\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.probability import *\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the pdf file to text and downloading the pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating an empty list named as `main_data`. After reading the data from the converted text file we need to\n",
    "make sure that everything comes clearly as the pdfminer will give lot of new lines between each element. So here we are \n",
    "replacing the newlines with an empty character and appending to the main_data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pdf2txt.py -o group_053.txt Group053.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_SYvSwkYSQG"
   },
   "outputs": [],
   "source": [
    "\n",
    "main_data = []  \n",
    "with open('group_053.txt') as f:\n",
    "    for line in f:\n",
    "        main_data.append(line.replace('\\n','')) #clearing the newlines and appending to the main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code we are reading the data from the main_data list and extracting pdf_name and the url provided for that \n",
    "particular pdf and storing these items as key value pairs in the dict_main dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZATBL-yhZiVI"
   },
   "outputs": [],
   "source": [
    "#test_string = main_data[4]  \n",
    "pdf_name = re.compile(r'([\\w]*(\\.pdf))')  #regex for getting the pdf id \n",
    "url = re.compile(r'((http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?)') #regex for extrating the pdf url\n",
    "dict_main = {}\n",
    "for line in main_data:  #serching through each item in the main_data list and storing the pdf_name and url values as keyvalue pairs in dic_main()\n",
    "    match1 = re.search(pdf_name,line)\n",
    "    match2 = re.search(url,line)\n",
    "    if match1 and match2:\n",
    "        dict_main[match1.group(1)] = match2.group(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have all the list of pdf file names and the url's in a dictionary, so we using that dictionary values to download the respective pdf documents\n",
    "and storing it with the pdf name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPB0Ok0dduku"
   },
   "outputs": [],
   "source": [
    "for file_name,url in dict_main.items():\n",
    "    data = requests.get(url, allow_redirects=True)\n",
    "    open(file_name, 'wb').write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the information from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1T69XaDaWoa"
   },
   "outputs": [],
   "source": [
    "\"\"\"Converting the pdf data into text for all downloaded pdf files\"\"\"\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block goes to the current working directory, open the pdf files and convert the data in the pdf to the text format\n",
    "and stores the file data and file name in the file_list and file_names_list respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()  # getting the currect working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [] #creating an empty list to store the data of each pdf file\n",
    "file_names_list = [] # creating an empty list to store the name of each pdf file \n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        file_re = re.search(r'(PP[\\w]+\\.pdf)',file) # regex for extracting the file.\n",
    "        file_names = re.search(r'(PP[\\w]+)',file) #regex for getting the particular file name\n",
    "        if file_re: \n",
    "            file_list.append(convert_pdf_to_txt(file_re.group(1))) # if the file exists in place then we are convertign to the text format\n",
    "            \n",
    "        if file_names:\n",
    "            file_names_list.append(file_names.group(1)) #appending the file names to the file_name list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting paper body from the file_list using `1 Paper Body(.*?)2\\sReferences` regular expression and storing into a list called `paper_body`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_body = []  #creating an empty list for storing the body of the pdf\n",
    "for each in file_list:\n",
    "    paper_regex = re.search(r'1 Paper Body(.*?)2\\sReferences', each, re.DOTALL)  ## regex for extracting the body of each pdf file\n",
    "    match_body = paper_regex.group(1).strip()\n",
    "    if paper_regex:\n",
    "        paper_body.append(match_body)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will convert the words whichever occuring at the starting of a line or a sentence to the lowercase. We do not convert the words ot lowercase which are occuring the in the middle of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is used to extract the sentences from the paragraphs.\n",
    "Number of arguments for this function is 1\"\"\"\n",
    "\n",
    "def sent_tokenizer(paragraphs):\n",
    "    return (sent_tokenize(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_normalise = []\n",
    "for each in paper_body:  ##looping through the body of each pdf\n",
    "    each_sentence = sent_tokenizer(each) #converting paragraphs to sentences and storing in the \n",
    "    sent_list=[]\n",
    "    for each in each_sentence:\n",
    "        each_list=each.split()\n",
    "        each_list[0] = each_list[0].lower()  # case normalization lower is executed here\n",
    "        joined = ' '.join(each_list)\n",
    "        sent_list.append(joined)\n",
    "    paperbody_sent = ' '.join(sent_list)\n",
    "    sentence_normalise.append(paperbody_sent) #storing the normalized sentences in `sentence_normalize` list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below task we are performing tokenization (breaking a character sequence into pieces is known as tokenization). \n",
    "The tokens created here are all unigrams, but these will contain many stopwords and indentifiers in it. We can say that tokenization gives us the unigram vocab with lot of unwanted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sentences (`sentence_normalise`) which we have generated in the previous code block, we extract each word as tokens and storing it in the `main_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "main_list = []\n",
    "for each in sentence_normalise:  \n",
    "    tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    unigram_tokens = tokenizer.tokenize(each) #extracting tokens by using the regex tokenizer\n",
    "    main_list.append(unigram_tokens) #appending the tokens to the main_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_text dictionary creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary named `raw_text` which stores the token values with respect to the pdf file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = {}\n",
    "for name,text in zip(file_names_list,main_list): #zipping through the pdf file names and the list of tokens that has been extracted\n",
    "    raw_text[name] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are the words that carry little lexical content. They are often functional words in English, for example, articles, pronouns, particles, and so on. In NLP and IR, we usually exclude stop words from the vocabulary. We are removing the stopwords which have been provided in the `stopwords_en.txt` text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the stopwords_en.txt file \n",
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    \n",
    "stopwordsSet = set(stopwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the stopwords.txt, we need to remove stopwords from 200 paper bodies which are there in the dictionary `raw_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in raw_text.items():   # removal of stopwords from each paper body in dictionary called raw_text\n",
    "    raw_text[k] =[w for w in v if w not in stopwordsSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Concatenating all the tokenized patents using the chain.frome_iterable function. The returned list `words`\n",
    "by the function contains a list of all the words seprated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  28611 \n",
      "Total number of tokens:  430844 \n",
      "Lexical diversity:  15.058683723043584\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "words = list(chain.from_iterable(raw_text.values())) #lopping through values combining them and storing as a list\n",
    "vocab = set(words) #getting only the unique values from the words list\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams Creation\n",
    "\n",
    "The next task is go generate the bigram collocations, given the tokenized patents.\n",
    "\n",
    "After removing stopwords from raw_text dictionary, we need to create bigram from the unigram tokens called the raw_text dictionary values.\n",
    "\n",
    "After getting a list of all tokens we will generate the 200 bigram cllocations. The functions you need include\n",
    "* BigramAssocMeasures()\n",
    "* BigramCollocationFinder.from_words()\n",
    "* apply_word_filter(lambda w: len(w) < 3)\n",
    "* nbest(bigram_measures.pmi, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams\n",
    "len(top_200_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task is to tokenise the paper body with only unigrams. Now, we introduce 200 collcations. we need to make sure those collocations are not split into two individual words.\n",
    "After creating the mwetokenizer we are creating a dictionary `colloc_tokens` than has the uni and bigram vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_bigrams,separator = '__')  ## creatign a MWEtokenizer with the bigrams created\n",
    "colloc_tokens =  dict((pdf, mwetokenizer.tokenize(tokens)) for pdf,tokens in raw_text.items()) #creating bigram vocab by checking the tokens that are present in th epdf file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28431\n"
     ]
    }
   ],
   "source": [
    "all_words_colloc = list(chain.from_iterable(colloc_tokens.values()))\n",
    "colloc_voc = list(set(all_words_colloc))\n",
    "print(len(colloc_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Context Dependent and Rare Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens whose frequency in documents appeared less than 3% is removed and the context dependent words with threshold to 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the frequency count of unigrams and bigrams\n",
    "fd_3 = FreqDist(all_words_colloc)\n",
    "less_freq_words = set([ k for k,v in fd_3.items() if v < 6 or v > 190])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the context dependent and rare tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if w not in less_freq_words\n",
    "for k,v in colloc_tokens.items():\n",
    "    #first_v = list(v)\n",
    "    colloc_tokens[k] = [ w for w in v if w not in less_freq_words]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### length less than 3 Tokens removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in colloc_tokens.items():\n",
    "    colloc_tokens[k] = [ w for w in v if len(w) >= 3]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk stemming(Porter Stemmer) is used to remove pluralisation of nouns, resulting in a decrease in the vocabulary - but not affecting the overall count of tokens. We do this stemming for the text analysis as we get root words for all the tokens we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmed_tokens = {}\n",
    "stemmer = PorterStemmer()  # stemming the using PorterStemmer()\n",
    "for key,value in colloc_tokens.items():\n",
    "    stemming_allgrams = []\n",
    "    for token in value:\n",
    "        if token == token.lower():\n",
    "            token=stemmer.stem(token)\n",
    "        else:\n",
    "            token= stemmer.stem(token)\n",
    "        stemming_allgrams.append(token)\n",
    "    stemmed_tokens[key] = stemming_allgrams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Vocabulary Index File "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the vocabulary index from the vocabulary that we have created by performing all the above steps. This file contains the words (features) in the sorted order with the index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = list(chain.from_iterable(stemmed_tokens.values()))\n",
    "vocabulary_set = sorted(set(vocabulary))\n",
    "\n",
    "with open('group053_vocab.txt','w', encoding=\"utf-8\") as file:    # opening the file here\n",
    "    for value in vocabulary_set:\n",
    "        vocab_inx = (f'{value}:{vocabulary_set.index(value)}\\n') \n",
    "        file.write(vocab_inx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sparse Count Vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a sparse vector from the vocabulary index we have, this vector cotains the count of vocab that is present in each pdf file. Since this is a sparse vector, we only get the value if the vocab is present in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('group053_count_vectors.txt','w',encoding=\"utf-8\") as vec:\n",
    "    for p in range(len(stemmed_tokens)):\n",
    "        name = file_names_list[p]\n",
    "        vec.write(f'{name}')\n",
    "       \n",
    "        #print(stemmed_tokens[name])\n",
    "        for each in stemmed_tokens[name]:\n",
    "            if each in vocabulary_set:\n",
    "                vec.write(f',{vocabulary_set.index(each)}:{stemmed_tokens[name].count(each)}')\n",
    "        vec.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Task 2, we are asked find statistics of Authors, Titles and Abstracts. To perform statistics, we need to do preprossing steps on the titles and abstracts.\n",
    "\n",
    "Inorder to do the text preprocessing, we need to first extract titles, authors and abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function called `title_extract` is defined to extract the titles of the pdf file. In this we use `(.*?)Authored by:` regular expression to extract the titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology for Task B:\n",
    "The following tasks has been performed in the task B\n",
    "1. Extracting the required Title, Abstract and Author names for the pdf data\n",
    "2. Normalizing the extracted data\n",
    "3. Tokenizing Title and Abstract\n",
    "4. Removing stopwords\n",
    "5. Calculate the frequency and getting required metrics\n",
    "6. Generating a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract titles of the papers\n",
    "def title_extract(text):\n",
    "    \"\"\"\n",
    "    Function name: title_extract\n",
    "    Number of arguments: 1\n",
    "    Arguments: text\n",
    "    Description: This function is used to extract the titles from the pdf file\n",
    "    Return value: Title of the pdf file\n",
    "    \"\"\"\n",
    "    titles_re = re.search(r'(.*?)Authored by:', text,re.DOTALL) # re expression to extract title of the paper\n",
    "    titles_re = titles_re.group(1).strip()   \n",
    "    if titles_re:\n",
    "        return (titles_re) # if match we return the title name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function called `abstract_extract` is defined to extract the abstracts of the pdf file. In this we use `Abstract(.*?)1 Paper Body` regular expression to extract the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the abstract\n",
    "def abstract_extract(text):\n",
    "    \"\"\"\n",
    "    Function name: abstract_extract\n",
    "    Number of arguments: 1\n",
    "    Arguments: text\n",
    "    Description: This function is used to extract the abstract from the pdf file\n",
    "    Return value: Abstract of the pdf file\n",
    "    \"\"\"\n",
    "    abstract_re = re.search(r'Abstract(.*?)1 Paper Body', text,re.DOTALL)  # regular expression to extract abstract\n",
    "    abstract_re = abstract_re.group(1).strip()\n",
    "    if abstract_re:      # if match we return the abstract of the paper\n",
    "        return abstract_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to extract author names from the raw file using `Authored by:(.*?)Abstract` regular expression and we get output of list of author names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "author_list_names = []\n",
    "for each in file_list:\n",
    "    author_re = re.search(r'Authored by:(.*?)Abstract', each,re.DOTALL)  ## regex for extracting the author names\n",
    "    author_re = author_re.group(1).strip()  \n",
    "    author_re = author_re.split('\\n')\n",
    "    author_list.append(author_re)\n",
    "for each in author_list:  #looping through the list of authornames and appending them to a list\n",
    "    for item in each:\n",
    "        if len(item) != 0:\n",
    "            author_list_names.append(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are calling the two functions defined above `title_extract` and `abstract_extract` to extract titles and abstracts from teh raw file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_title = []\n",
    "extracted_abstract = []\n",
    "for each in file_list:\n",
    "    extracted_title.append(title_extract(each))  #function call to extract title\n",
    "    extracted_abstract.append(abstract_extract(each)) #function call to extract abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Titles and Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalise titles and abstracts before tokenization. For normalizing titles we use `lower()` method to normalise titles as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_title = []\n",
    "for each in extracted_title:\n",
    "    each = each.lower()\n",
    "    normalised_title.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_normalise = []\n",
    "for each in extracted_abstract:\n",
    "    each_sent = sent_tokenizer(each)\n",
    "    sent_list=[]\n",
    "    for each in each_sent:\n",
    "        each_list=each.split()\n",
    "        each_list[0] = each_list[0].lower()  # case normalization lower is executed here\n",
    "        joined = ' '.join(each_list)\n",
    "        sent_list.append(joined)\n",
    "    abstract_sent = ' '.join(sent_list)\n",
    "    abstract_normalise.append(abstract_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The word tokenization must use the following regular expression, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenization(text,a):\n",
    "    if a == 0:\n",
    "        tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "        titles = tokenizer.tokenize(text)\n",
    "        return titles\n",
    "    else:\n",
    "        tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "        abstract = tokenizer.tokenize(text)\n",
    "        return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_title = []\n",
    "paper_abstract = []\n",
    "# tokenizing title\n",
    "for each in normalised_title:\n",
    "    tokenized_title = regex_tokenization(each,0)\n",
    "    paper_title.append(tokenized_title)\n",
    "        \n",
    "# tokenizing abstract\n",
    "for text in abstract_normalise:\n",
    "    tokenized_abstract = regex_tokenization(text,1)\n",
    "    paper_abstract.append(tokenized_abstract)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context-independent stop words (i.e, stopwords_en.txt) is being removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_stopwords = []\n",
    "for each in paper_title:\n",
    "    title_stopwords.append([w for w in each if w not in stopwordsSet])\n",
    "    \n",
    "abstract_stopwords = []\n",
    "for each in paper_abstract:\n",
    "    abstract_stopwords.append([w for w in each if w not in stopwordsSet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top most Frequent words in Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_authors = FreqDist(author_list_names)\n",
    "most_comm_authors = fd_authors.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top most Frequent words in Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens = list(chain.from_iterable(title_stopwords))\n",
    "fd_titles = FreqDist(title_tokens)\n",
    "most_comm_titles = fd_titles.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top most Frequent words in Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = list(chain.from_iterable(abstract_stopwords))\n",
    "fd_abstract = FreqDist(abstract_tokens)\n",
    "most_comm_abstract = fd_abstract.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_freq_words(my_list):\n",
    "    k,v = zip(*my_list)\n",
    "    my_freq_words = list(k)\n",
    "    return my_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_abstract = extract_freq_words(most_comm_abstract)\n",
    "freq_words_titles = extract_freq_words(most_comm_titles)\n",
    "freq_words_authors = extract_freq_words(most_comm_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe to append all the top 10 frequent words in titles, abstract and authors.\n",
    "freq_df = pd.DataFrame()\n",
    "freq_df['top10_terms_in_abstracts'] = freq_words_abstract  # appending list of top 10 abstracts to column called top10_terms_in_abstracts\n",
    "freq_df['top10_terms_in_titles'] = freq_words_titles   # appending list of  top 10 titles to column called top10_terms_in_titles\n",
    "freq_df['top10_authors'] = freq_words_authors       # appending list of  top 10 authors to column called top10_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>Xi Chen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>models</td>\n",
       "      <td>Han Liu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>regression</td>\n",
       "      <td>Martin J. Wainwright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>networks</td>\n",
       "      <td>Ambuj Tewari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem</td>\n",
       "      <td>inference</td>\n",
       "      <td>Eric P. Xing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>show</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>Trevor Darrell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>models</td>\n",
       "      <td>modeling</td>\n",
       "      <td>Bernhard Sch?lkopf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>approach</td>\n",
       "      <td>gradient</td>\n",
       "      <td>Jonathan W. Pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>method</td>\n",
       "      <td>sparse</td>\n",
       "      <td>Larry Wasserman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>estimation</td>\n",
       "      <td>Robert C. Williamson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_terms_in_abstracts top10_terms_in_titles         top10_authors\n",
       "0                 learning              learning               Xi Chen\n",
       "1                     data                models               Han Liu\n",
       "2                algorithm            regression  Martin J. Wainwright\n",
       "3                    model              networks          Ambuj Tewari\n",
       "4                  problem             inference          Eric P. Xing\n",
       "5                     show            stochastic        Trevor Darrell\n",
       "6                   models              modeling    Bernhard Sch?lkopf\n",
       "7                 approach              gradient    Jonathan W. Pillow\n",
       "8                   method                sparse       Larry Wasserman\n",
       "9               algorithms            estimation  Robert C. Williamson"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the dataframe to csv file \n",
    "freq_df.to_csv(r'group053_stats.csv',index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FIT5196_ass2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
